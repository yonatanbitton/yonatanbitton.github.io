<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  <meta name="author" content="Yonatan Bitton" />
  <meta name="description" content="Research Scientist at Google, CS PhD" />
  <link rel="alternate" hreflang="en-us" href="https://yonatanbitton.github.io/" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <meta name="theme-color" content="#1565c0" />
  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
  <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="/css/wowchemy.481af39c39ffd87b2d14f39943e7c723.css" />
  <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Home" />
  <link rel="manifest" href="/manifest.webmanifest" />
  <link rel="icon" type="image/png" href="/media/icon_hu36fe5d02a7a8c0153faf9a2476a5266c_16606_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu36fe5d02a7a8c0153faf9a2476a5266c_16606_180x180_fill_lanczos_center_3.png" />
  <link rel="canonical" href="https://yonatanbitton.github.io/" />

  <meta property="twitter:card" content="summary" />
  <meta property="og:site_name" content="Home" />
  <meta property="og:url" content="https://yonatanbitton.github.io/" />
  <meta property="og:title" content="Home" />
  <meta property="og:description" content="Research Scientist at Google, CS PhD" /><meta property="og:image" content="https://yonatanbitton.github.io/media/icon_hu36fe5d02a7a8c0153faf9a2476a5266c_16606_512x512_fill_lanczos_center_3.png" />
  <meta property="twitter:image" content="https://yonatanbitton.github.io/media/icon_hu36fe5d02a7a8c0153faf9a2476a5266c_16606_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  <meta property="og:updated_time" content="2030-06-01T13:00:00&#43;00:00" />


  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "WebSite",
      "potentialAction": {
        "@type": "SearchAction",
        "target": "https://yonatanbitton.github.io/?q={search_term_string}",
        "query-input": "required name=search_term_string"
      },
      "url": "https://yonatanbitton.github.io/"
    }
  </script>

  <title>Home</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper   "  >


  <script src="/js/wowchemy-init.min.8988fb2a4bba758785868cfcb5244555.js"></script>


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">

        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">

      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">

      </div>

    </section>
  </div>
</aside>



  <div class="page-header">



<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">


    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Home</a>
    </div>



    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>



    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Home</a>
    </div>



    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">


      <ul class="navbar-nav d-md-inline-flex">

        <li class="nav-item">
          <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
        </li>

        <li class="nav-item">
        <a class="nav-link" href="/#papers-by-venue" data-target="#papers-by-venue">
          <span>Papers&nbsp;by&nbsp;Venue</span>
        </a>
      </li>

        <li class="nav-item">
          <a class="nav-link " href="/#publications" data-target="#publications"><span>Publications</span></a>
        </li>


        <li class="nav-item">
          <a class="nav-link " href="/#experience" data-target="#experience"><span>Experience</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#talks" data-target="#talks"><span>Talks</span></a>
        </li>


      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">


      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>

      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>



    </ul>

  </div>
</nav>

  </div>

  <div class="page-body">


<span class="js-widget-page d-none"></span>































































  <section id="about" class="home-section wg-about  "  >
   <div class="home-section-bg " >

   </div>
    <div class="container">



















<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">



      <img class="avatar avatar-circle"
           width="270" height="270"
           src="/authors/admin/avatar_hub6a82f0e24dfc8eb87466fed56bca441_709630_270x270_fill_q75_lanczos_center.jpg" alt="Yonatan Bitton">


      <div class="portrait-title">
        <h2>Yonatan Bitton</h2>
        <h3>Senior Research Scientist at Google, CS PhD</h3>


        <h3>
          <a href="http://www.huji.ac.il/" target="_blank" rel="noopener">
          <span>The Hebrew University of Jerusalem</span>
          </a>
        </h3>

      </div>

      <ul class="network-icon" aria-hidden="true">










        <li>
          <a href="mailto:yonatanbitton1@gmail.com"  aria-label="envelope">
            <i class="fas fa-envelope big-icon"></i>
          </a>
        </li>












        <li>
          <a href="https://twitter.com/YonatanBitton" target="_blank" rel="noopener" aria-label="twitter">
            <i class="fab fa-twitter big-icon"></i>
          </a>
        </li>












        <li>
          <a href="https://scholar.google.com/citations?user=P9Fpf4sAAAAJ&amp;hl=en" target="_blank" rel="noopener" aria-label="graduation-cap">
            <i class="fas fa-graduation-cap big-icon"></i>
          </a>
        </li>












        <li>
          <a href="https://github.com/yonatanbitton" target="_blank" rel="noopener" aria-label="github">
            <i class="fab fa-github big-icon"></i>
          </a>
        </li>












        <li>
          <a href="https://www.linkedin.com/in/yonatan-bitton-5ba681149/" target="_blank" rel="noopener" aria-label="linkedin">
            <i class="fab fa-linkedin big-icon"></i>
          </a>
        </li>

      </ul>

    </div>
  </div>
  <div class="col-12 col-lg-8">


    <h1>Biography</h1>

     <div class="article-style">
      <p>I am a Research Scientist at <a href="https://ai.google/" target="_blank" rel="noopener">Google Research</a> in Tel-Aviv where I work on multimodal consistency.
          </p>
<p>My research is centered on improving large vision-and-language models. I develop feedback models for text-to-image and text-to-video applications, specifically designed to enhance the alignment of visual outputs with their corresponding textual prompts. Additionally, I work on multimodal factuality, including visual understanding and image or video-to-text evaluation, ensuring that the generated text is factually correct and attributable to trustworthy textual or visual sources.</p>

<!--<p>The goal of my research is to improve vision and language generalization. Specifically, I aim to develop models with better compositionality abilities, less biased and better perform on real-world examples. My recent works and interest areas include <strong>image-text alignment, improving text-to-image models, and visual instruction tuning</strong>. See my publications for more details. -->
<!--    </p>-->
    <p>I completed my PhD in <a href="http://www.huji.ac.il/" target="_blank" rel="noopener">The Hebrew University of Jerusalem, Israel</a>. During my time there, I had the privilege of being advised by <a href="https://schwartz-lab-huji.github.io/" target="_blank" rel="noopener">Dr. Roy Schwartz</a> and <a href="https://gabrielstanovsky.github.io/" target="_blank" rel="noopener">Dr. Gabriel Stanovsky</a>. My PhD talk "Bridging Vision and Language with Data: From Perception to Understanding" 🎬 record is available <a href="https://www.youtube.com/watch?v=SR-od_Qsnvs">here</a>.
          I did my MSc with <a href="http://www.cs.bgu.ac.il/~elhadad" target="_blank" rel="noopener">Prof. Michael Elhadad</a> and <a href="http://www.cs.bgu.ac.il/~ebachmat/" target="_blank" rel="noopener">Prof. Eitan Bachmat</a>, at the Ben Gurion University.</p>
  <p></p>
<!--        <i class="fas fa-download  pr-1 fa-fw"></i> Download my complete <a href="/uploads/cv.pdf" target="_blank">CV</a>. -->
            <i class="fas fa-download pr-1 fa-fw"></i> Download my complete CV: <a href="/uploads/cv.pdf" target="_blank">link</a>.<br>
            📄 Download my bio: <a href="/uploads/bio.txt" target="_blank">link</a>.
     </p>

    </div>


    <div class="row">

      <div class="col-md-7">
        <div class="section-subheading">Education</div>
        <ul class="ul-edu fa-ul mb-0">

         <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PhD in Computer Science (Vision-and-Language), 2020-2023</p>
              <p class="institution">The Hebrew University of Jerusalem, Israel</p>
            </div>
          </li>

          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">MSc in Computer Science (Natural Language Processing), Magna cum laude, 2018-2019</p>
              <p class="institution">Ben Gurion University of the Negev, Israel</p>
            </div>
          </li>

          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">BSc in Computer Science, 2015-2018</p>
              <p class="institution">Ben Gurion University of the Negev, Israel</p>
            </div>
          </li>

        </ul>
      </div>
      </div>


<h3>Students</h3>
<p>
    I've had the opportunity to collaborate with several MSc and PhD students towards their publication goals:
</p>

<ul class="ul-edu fa-ul mb-0">
   <li>
    <strong><a href="https://gordonhu608.github.io/" target="_blank" rel="noopener">Wenbo (Gordon) Hu</a> (UCLA University)</strong> – 
    <a href="https://3dllm-mem.github.io/" target="_blank" rel="noopener">3DLLM-Mem</a>: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model
  </li>
  <li>
    <li>
    <strong><a href="https://scholar.google.com/citations?hl=en&user=VNDhTycAAAAJ" target="_blank" rel="noopener">Brian Gordon</a> (Tel-Aviv University)</strong> –
    <ul>
      <li><a href="https://mismatch-quest.github.io/" target="_blank" rel="noopener">Mismatch Quest:</a> Visual and Textual Feedback for Image-Text Misalignment</li>
      <li><a href="https://google.github.io/unblocking-detail-caption/" target="_blank" rel="noopener">Unblocking Fine-Grained Evaluation of Detailed Captions:</a> An Explaining AutoRater and Critic-and-Revise Pipeline</li>
    </ul>
  </li>
  <strong>
    <a href="https://scholar.google.com/citations?user=oAy77cgAAAAJ&hl=en" target="_blank" rel="noopener">
      Aviv Slobodkin
    </a> (Bar-Ilan University)
  </strong> – 
  <a href="https://arxiv.org/abs/2504.17502" target="_blank" rel="noopener">
    RefVNLI:
  </a> Towards Scalable Evaluation of Subject-driven Text-to-image Generation
</li>
  <li>
    <strong><a href="https://scholar.google.com/citations?user=ZsXf6OMAAAAJ&hl=en" target="_blank" rel="noopener">Moran Yanuka</a> (Tel Aviv University)</strong> - <a href="https://arxiv.org/abs/2411.09018v1" target="_blank" rel="noopener">Bridging the Visual Gap:</a> Fine-Tuning Multimodal Models with Knowledge-Adapted Captions
  </li>
  <li>
    <strong><a href="https://venturamor.github.io/" target="_blank" rel="noopener">Mor Ventura</a> (Technion)</strong> - <a href="https://venturamor.github.io/NLEye/" target="_blank" rel="noopener">NL-Eye:</a> Abductive NLI for Images
  </li>
    <li>
        <strong><a href="https://orrzohar.github.io/" target="_blank" rel="noopener">Orr Zohar</a> (Stanford)</strong> - <a href="https://orrzohar.github.io/projects/video-star/" target="_blank" rel="noopener">Video-STaR</a>: Bootstrapping Weak Video Supervision for Visual Instruction Tuning
    </li>
    <li>
        <strong><a href="https://sites.google.com/view/hbansal" target="_blank" rel="noopener">Hritik Bansal</a> (UCLA)</strong> -
        <ul>
            <li><a href="https://videophy2.github.io/" target="_blank" rel="noopener">VideoPhy2:</a> Challenging Action-Centric Physical Commonsense Evaluation of Video Generation</li>
            <li><a href="https://videophy.github.io/" target="_blank" rel="noopener">VideoPhy:</a> Evaluating Physical Commonsense In Video Generation</li>
            <li><a href="https://talc-mst2v.github.io/" target="_blank" rel="noopener">TALC:</a> Time-Aligned Captions for Multi-Scene Text-to-Video Generation</li>
            <li><a href="https://video-con.github.io/" target="_blank" rel="noopener">Video-Con:</a> Robust Video-Language Alignment via Contrast Captions</li>
        </ul>
    </li>
    <li>
        <strong><a href="https://scholar.google.com/citations?user=bIV6JQ0AAAAJ&hl=en" target="_blank" rel="noopener">Nitzan Bitton-Guetta</a> (Ben Gurion)</strong> -
        <ul>
            <li><a href="https://whoops-benchmark.github.io/" target="_blank" rel="noopener">WHOOPS!</a> Commonsense-defying image with text-to-image models</li>
            <li><a href="https://visual-riddles.github.io/" target="_blank" rel="noopener">Visual Riddles</a>: a Commonsense and World Knowledge Challenge for Large Vision and Language Models</li>
        </ul>
    </li>
  <li>
    <strong><a href="https://scholar.google.com/citations?hl=en&user=mhdOCG4AAAAJ" target="_blank" rel="noopener">Ron Yosef</a> (Hebrew University)</strong> –
    <ul>
      <li><a href="https://arxiv.org/abs/2303.15445" target="_blank" rel="noopener">IRFL:</a> Figurative language and visual metaphors</li>
      <li><a href="https://editinspector.github.io/" target="_blank" rel="noopener">EditInspector:</a> A Benchmark for Evaluation of Text-Guided Image Edits</li>
    </ul>
  </li>
     <li>
          <strong><a href="https://scholar.google.com/citations?hl=en&user=KHBd12kAAAAJ" target="_blank" rel="noopener">Oren Sultan</a> (Hebrew University)</strong> - <a href="https://arxiv.org/abs/2403.01139" target="_blank" rel="noopener">ParallelPARC:</a> A Scalable Pipeline for Generating Natural-Language Analogies
      </li>
  <li>
    <strong>Netta Madvil (Hebrew University)</strong> - <a href="https://arxiv.org/abs/2307.04532" target="_blank" rel="noopener">Read, Look or Listen?:</a> Multimodal models analysis
  </li>
</ul>
    
<p>
    If you want to work together on vision-and-language research, feel free to shoot me an email.
</p>

    </div>
  </section>

  <!-- ───────────────────────────────────────────────────────────────
     PAPERS BY VENUE  (3-column grid • collapsible • year badges)
     ─────────────────────────────────────────────────────────── -->
<section id="papers-by-venue" class="home-section wg-papers-by-venue">
  <style>
    /* ——— Layout: 3 columns on ≥768 px ——— */
    #papers-by-venue .year-col        { padding:0 .75rem 1.5rem }
    @media (min-width:768px){         /* md */
      #papers-by-venue .year-grid     { display:flex; flex-wrap:wrap }
      #papers-by-venue .year-col      { flex:0 0 33.333%; max-width:33.333% }
    }
    /* ——— Typography & visuals ——— */
    #papers-by-venue h3.year-heading  { font-size:1.35rem; margin-bottom:.25rem }
    #papers-by-venue details          { margin:.15rem 0 }
    #papers-by-venue summary          { cursor:pointer; user-select:none }
    #papers-by-venue .venue-badge     { font-size:0.75rem; margin-left:.35rem }
    #papers-by-venue .paper-link      { display:block; padding:.1rem 0 .1rem 1.1rem }
    #papers-by-venue .paper-link:hover{ text-decoration:underline }
    /* ——— Year-level badge (orange) ——— */
    #papers-by-venue .year-badge      {
        background:#ff9800;           /* Wow-factor orange */
        color:#fff;
        font-size:.80rem;
        vertical-align:middle;
        margin-left:.45rem;
    }
  </style>

  <div class="home-section-bg"></div>
  <div class="container">

    <!-- headline -->
    <div class="section-heading">
      <h1>Papers by Venue</h1>
      <p class="text-muted mb-4">24&nbsp;peer-reviewed papers · 2021 – 2025</p>
    </div>

    <!-- year grid ------------------------------------------------->
    <div class="year-grid">

      <!-- 2025 *************************************************** -->
      <div class="year-col">
        <h3 class="year-heading">
          2025 <span class="badge year-badge">5</span>
        </h3>

        <details>
          <summary>NAACL <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://arxiv.org/abs/2506.xxxxx" target="_blank">
            Bridging&nbsp;the&nbsp;Visual&nbsp;Gap
          </a>
        </details>

        <details>
          <summary>ICLR <span class="badge badge-primary venue-badge">3</span></summary>
          <a class="paper-link" href="https://venturamor.github.io/NLEye/" target="_blank">NL-Eye</a>
          <a class="paper-link" href="https://orrzohar.github.io/projects/video-star/" target="_blank">VideoSTaR</a>
          <a class="paper-link" href="https://videophy.github.io/" target="_blank">VideoPhy</a>
        </details>

        <details>
          <summary>WACV <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://arxiv.org/abs/2407.11814" target="_blank">
            Contrastive&nbsp;Sequential-Diffusion&nbsp;Learning
          </a>
        </details>
      </div>

      <!-- 2024 *************************************************** -->
      <div class="year-col">
        <h3 class="year-heading">
          2024 <span class="badge year-badge">10</span>
        </h3>

        <details><summary>AIES  <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://arxiv.org/abs/2406.16807" target="_blank">
            Beyond&nbsp;Thumbs&nbsp;Up/Down
          </a>
        </details>

        <details><summary>EMNLP <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://arxiv.org/abs/2405.02793" target="_blank">
            ImageInWords
          </a>
        </details>

        <details><summary>NeurIPS <span class="badge badge-primary venue-badge">2</span></summary>
          <a class="paper-link" href="https://www.datacomp.ai/dclm/" target="_blank">DataComp-LM</a>
          <a class="paper-link" href="https://visual-riddles.github.io/"  target="_blank">Visual&nbsp;Riddles</a>
        </details>

        <details><summary>ECCV <span class="badge badge-primary venue-badge">2</span></summary>
          <a class="paper-link" href="https://google.github.io/docci" target="_blank">DOCCI</a>
          <a class="paper-link" href="https://mismatch-quest.github.io/" target="_blank">Mismatch&nbsp;Quest</a>
        </details>

        <details><summary>CVPR <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://video-con.github.io/" target="_blank">VideoCon</a>
        </details>

        <details><summary>ACL <span class="badge badge-primary venue-badge">2</span></summary>
          <a class="paper-link" href="https://arxiv.org/abs/2402.00559" target="_blank">Chain-of-Thought&nbsp;Verifier</a>
          <a class="paper-link" href="https://arxiv.org/abs/2405.10122" target="_blank">Visual&nbsp;Illustrations&nbsp;Sequences</a>
        </details>

        <details><summary>NAACL <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://arxiv.org/abs/2403.01139" target="_blank">ParallelPARC</a>
        </details>
      </div>

      <!-- 2023 *************************************************** -->
      <div class="year-col">
        <h3 class="year-heading">
          2023 <span class="badge year-badge">6</span>
        </h3>

        <details><summary>NeurIPS <span class="badge badge-primary venue-badge">4</span></summary>
          <a class="paper-link" href="https://visit-bench.github.io/" target="_blank">VisIT-Bench</a>
          <a class="paper-link" href="https://www.datacomp.ai/" target="_blank">DataComp</a>
          <a class="paper-link" href="https://laion.ai/blog/open-flamingo/" target="_blank">OpenFlamingo</a>
          <a class="paper-link" href="https://wysiwyr-itm.github.io/" target="_blank">WYSIWYR</a>
        </details>

        <details><summary>EMNLP <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://question2dialog.github.io/" target="_blank">q2d</a>
        </details>

        <details><summary>ICCV <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://whoops-benchmark.github.io/" target="_blank">WHOOPS!</a>
        </details>
      </div>

      <!-- 2022 *************************************************** -->
      <div class="year-col">
        <h3 class="year-heading">
          2022 <span class="badge year-badge">1</span>
        </h3>

        <details><summary>NeurIPS <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://winogavil.github.io/" target="_blank">WinoGAViL</a>
        </details>
      </div>

      <!-- 2021 *************************************************** -->
      <div class="year-col">
        <h3 class="year-heading">
          2021 <span class="badge year-badge">2</span>
        </h3>

        <details><summary>EMNLP <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://arxiv.org/abs/2109.02040" target="_blank">
            Data&nbsp;Efficient&nbsp;MLM&nbsp;for&nbsp;V&amp;L
          </a>
        </details>

        <details><summary>NAACL <span class="badge badge-primary venue-badge">1</span></summary>
          <a class="paper-link" href="https://arxiv.org/abs/2103.09591" target="_blank">
            Auto&nbsp;Contrast&nbsp;Sets&nbsp;from&nbsp;Scene&nbsp;Graphs
          </a>
        </details>
      </div>

    </div><!-- /year-grid -->

  </div><!-- /container -->
</section>
<!-- ───────────────────────────────────────────────────────────── -->


  <section id="publications" class="home-section wg-featured  "  >
    <div class="home-section-bg " >
    </div>
    <div class="container">
      <div class="row  ">
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Publications</h1>
        </div>
        <div class="col-12 col-lg-8">

          <!-- New publication entry for Unblocking Fine-Grained Evaluation of Detailed Captions -->
<div class="media stream-item">
  <div class="mr-3">
    <div class="img-hover-zoom">
      <img src="/publication/unblocking-detail-caption.png" class="article-banner" alt="Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline" loading="lazy">
    </div>
  </div>
  <div class="media-body">
    <div class="section-subheading article-title">
      <a href="https://www.arxiv.org/abs/2506.07631" target="_blank" rel="noopener">Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline</a>
    </div>
    <div class="stream-meta article-metadata">
      <div class="article-metadata">
        <div>
          <strong><span>Brian Gordon*</span></strong>,
          <strong><span>Yonatan Bitton*</span></strong>,
          <span>Andreea Marzoca</span>,
          <span>Yasumasa Onoe</span>,
          <span>Xiao Wang</span>,
          <span>Daniel Cohen-Or</span>,
          <span>Idan Szpektor</span>
        </div>
        <span class="article-date">
          June 2025
        </span>
        <span class="middot-divider"></span>
        <span class="pub-publication">
          <em><strong>arXiv preprint</strong></em>
        </span>
        <div class="btn-links">
          <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.arxiv.org/abs/2506.07631" target="_blank" rel="noopener">
            arXiv
          </a>
          <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://google.github.io/unblocking-detail-caption/" target="_blank" rel="noopener">
            Project Website
          </a>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- New publication entry for 3DLLM-Mem -->
<div class="media stream-item">
  <div class="mr-3">
    <div class="img-hover-zoom">
      <img src="/publication/3dllm-mem.png" class="article-banner" alt="3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model" loading="lazy">
    </div>
  </div>
  <div class="media-body">
    <div class="section-subheading article-title">
      <a href="https://arxiv.org/abs/2505.22657" target="_blank" rel="noopener">3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</a>
    </div>
    <div class="stream-meta article-metadata">
      <div class="article-metadata">
        <div>
          <span>Wenbo Hu</span>,
          <span>Yining Hong</span>,
          <span>Yanjun Wang</span>,
          <span>Leison Gao</span>,
          <span>Zibu Wei</span>,
          <span>Xingcheng Yao</span>,
          <span>Nanyun Peng</span>,
          <strong><span>Yonatan Bitton</span></strong>,
          <span>Idan Szpektor</span>,
          <span>Kai-Wei Chang</span>
        </div>
        <span class="article-date">
          May 2025
        </span>
        <span class="middot-divider"></span>
        <span class="pub-publication">
          <em><strong>arXiv preprint</em></strong> <font color="purple">🎉 Best Paper Award at the Foundation Models Meet Embodied Agents Workshop @ CVPR 2025</font>
        </span>
        <div class="btn-links">
          <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2505.22657" target="_blank" rel="noopener">
            arXiv
          </a>
          <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://3dllm-mem.github.io/" target="_blank" rel="noopener">
            Project Website
          </a>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- New publication entry for EditInspector -->
<div class="media stream-item">
  <div class="mr-3">
    <div class="img-hover-zoom">
      <img src="/publication/edit-inspector.png"
           class="article-banner"
           alt="EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits"
           loading="lazy">
    </div>
  </div>
  <div class="media-body">
    <div class="section-subheading article-title">
      <a href="https://arxiv.org/abs/2506.09988"
         target="_blank"
         rel="noopener">
        EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits
      </a>
    </div>
    <div class="stream-meta article-metadata">
      <div class="article-metadata">
        <div>
          <span>Ron Yosef</span>,
          <span>Moran Yanuka</span>,
          <strong><span>Yonatan Bitton</span></strong>,
          <span>Dani Lischinski</span>
        </div>
        <span class="article-date">
          June 2025
        </span>
        <span class="middot-divider"></span>
        <span class="pub-publication">
          <em><strong>arXiv preprint</strong></em>
        </span>
        <div class="btn-links">
          <a class="btn btn-outline-primary btn-page-header btn-sm"
             href="https://arxiv.org/abs/2506.09988"
             target="_blank"
             rel="noopener">
            arXiv
          </a>
          <a class="btn btn-outline-primary btn-page-header btn-sm"
             href="https://editinspector.github.io/"
             target="_blank"
             rel="noopener">
            Project Website
          </a>
        </div>
      </div>
    </div>
  </div>
</div>

                    <div class="media stream-item">
            <div class="mr-3">
                <div class="img-hover-zoom">
                    <img src="/publication/refvnli.png" class="article-banner" alt="RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation" loading="lazy">
                </div>
            </div>
            <div class="media-body">
                <div class="section-subheading article-title">
                    <a href="https://arxiv.org/abs/2504.17502" target="_blank" rel="noopener">RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation</a>
                </div>
                <div class="stream-meta article-metadata">
                    <div class="article-metadata">
                        <div>
                            <span>Aviv Slobodkin</span>,
                            <span>Hagai Taitelbaum</span>,
                            <strong><span>Yonatan Bitton</span></strong>,
                            <span>Brian Gordon</span>,
                            <span>Michal Sokolik</span>,
                            <span>Nitzan Bitton Guetta</span>,
                            <span>Almog Gueta</span>,
                            <span>Royi Rassin</span>,
                            <span>Itay Laish</span>,
                            <span>Dani Lischinski</span>,
                            <span>Idan Szpektor</span>
                        </div>
                        <span class="article-date">
                            April 2025
                        </span>
                        <span class="middot-divider"></span>
                        <span class="pub-publication">
                            <em><strong>arXiv preprint</em></strong>
                        </span>
                        <div class="btn-links">
                            <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2504.17502" target="_blank" rel="noopener">
                                arXiv
                            </a>
                            <!-- Add other links like Project Website, Code, PDF if available -->
                            <!-- Example:
                            <a class="btn btn-outline-primary btn-page-header btn-sm" href="PROJECT_WEBSITE_URL" target="_blank" rel="noopener">
                                Project Website
                            </a>
                            -->
                        </div>
                    </div>
                </div>
            </div>
          </div>

          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/videophy2.png" class="article-banner" alt="VideoPhy2: Challenging Action-Centric Physical Commonsense Evaluation of Video Generation" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2503.06800">VideoPhy2: Challenging Action-Centric Physical Commonsense Evaluation of Video Generation</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Hritik Bansal*</span>,
                    <span>Clark Peng*</span>,
                    <strong><span>Yonatan Bitton*</span></strong>,
                    <span>Roman Goldenberg</span>,
                    <span>Aditya Grover</span>,
                    <span>Kai-Wei Chang</span>,
                </div>
                <span class="article-date">
                    March 2025
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>arXiv preprint</em></strong> <font color="purple">🎉 Best Paper Award at the Building Physically Plausible World Models Workshop @ ICML 2025</font>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2503.06800" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://videophy2.github.io/" target="_blank" rel="noopener">
                        Project Website
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>

          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/paligemma2.png" class="article-banner" alt="PaliGemma 2: A Family of Versatile VLMs for Transfer" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2412.03555">PaliGemma 2: A Family of Versatile VLMs for Transfer</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Andreas Steiner</span>,
                    <span>André Susano Pinto</span>,
                    <span>Michael Tschannen</span>,
                    <span>Daniel Keysers</span>,
                    <span>Xiao Wang</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Alexey Gritsenko</span>,
                    <span>Matthias Minderer</span>,
                    <span>Anthony Sherbondy</span>,
                    <span>Shangbang Long</span>,
                    <span>Siyang Qin</span>,
                    <span>Reeve Ingle</span>,
                    <span>Emanuele Bugliarello</span>,
                    <span>Sahar Kazemzadeh</span>,
                    <span>Thomas Mesnard</span>,
                    <span>Ibrahim Alabdulmohsin</span>,
                    <span>Lucas Beyer</span>,
                    <span>Xiaohua Zhai</span>
                </div>
                <span class="article-date">
                    December 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>arXiv preprint</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2412.03555" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://developers.googleblog.com/en/introducing-paligemma-2-powerful-vision-language-models-simple-fine-tuning/" target="_blank" rel="noopener">
                        Blog
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/blog/paligemma2" target="_blank" rel="noopener">
                        HuggingFace
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>


          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/visual_gap.png" class="article-banner" alt="Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2411.09018v1">Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Moran Yanuka</span>,
                    <span>Assaf Ben Kish</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Idan Szpektor</span>,
                    <span>Raja Giryes</span>
                </div>
                <span class="article-date">
                    November 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>NAACL 2025</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2411.09018v1" target="_blank" rel="noopener">
                        arXiv
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>


          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/kitten.png" class="article-banner" alt="KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2410.11824">KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Hsin-Ping Huang</span>,
                    <span>Xinyi Wang</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Hagai Taitelbaum</span>,
                    <span>Gaurav Singh Tomar</span>,
                    <span>Ming-Wei Chang</span>,
                    <span>Xuhui Jia</span>,
                    <span>Kelvin C.K. Chan</span>,
                    <span>Hexiang Hu</span>,
                    <span>Yu-Chuan Su</span>,
                    <span>Ming-Hsuan Yang</span>
                </div>
                <span class="article-date">
                    October 2024
                </span>
                 <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>arXiv preprint</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2410.11824" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    </div>
            </div>
        </div>
    </div>
</div>


          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/nleye.png" class="article-banner" alt="NL-Eye: Abductive NLI for Images" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://venturamor.github.io/NLEye/">NL-Eye: Abductive NLI for Images</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Mor Ventura</span>,
                    <span>Michael Toker</span>,
                    <span>Nitay Calderon</span>,
                    <span>Zorik Gekhman</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Roi Reichart</span>
                </div>
                <span class="article-date">
                    October 2024
                </span>
                 <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>ICLR 2025</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2410.02613" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://venturamor.github.io/NLEye/" target="_blank" rel="noopener">
                        Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/venturamor/nl_eye" target="_blank" rel="noopener">
                        Code
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/MorVentura/NL-Eye" target="_blank" rel="noopener">
                        Dataset
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>

  


  <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/datacomp_lm.png" class="article-banner" alt="DataComp-LM: In search of the next generation of training sets for language models" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2406.11794">DataComp-LM: In search of the next generation of training sets for language models</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Jeffrey Li</span>,
                    <span>Alex Fang</span>,
                    <span>Georgios Smyrnis</span>,
                    <span>Maor Ivgi</span>,
                    <span>Matt Jordan</span>,
                    <span>Samir Gadre</span>,
                    <span>Hritik Bansal</span>,
                    <span>Etash Guha</span>,
                    <span>Sedrick Keh</span>,
                    <span>Kushal Arora</span>,
                    <span>Saurabh Garg</span>,
                    <span>Rui Xin</span>,
                    <span>Niklas Muennighoff</span>,
                    <span>Reinhard Heckel</span>,
                    <span>Jean Mercat</span>,
                    <span>Mayee Chen</span>,
                    <span>Suchin Gururangan</span>,
                    <span>Mitchell Wortsman</span>,
                    <span>Alon Albalak</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Marianna Nezhurina</span>,
                    <span>Amro Abbas</span>,
                    <span>Cheng-Yu Hsieh</span>,
                    <span>Dhruba Ghosh</span>,
                    <span>Josh Gardner</span>,
                    <span>Maciej Kilian</span>,
                    <span>Hanlin Zhang</span>,
                    <span>Rulin Shao</span>,
                    <span>Sarah Pratt</span>,
                    <span>Sunny Sanyal</span>,
                    <span>Gabriel Ilharco</span>,
                    <span>Giannis Daras</span>,
                    <span>Kalyani Marathe</span>,
                    <span>Aaron Gokaslan</span>,
                    <span>Jieyu Zhang</span>,
                    <span>Khyathi Chandu</span>,
                    <span>Thao Nguyen</span>,
                    <span>Igor Vasiljevic</span>,
                    <span>Sham Kakade</span>,
                    <span>Shuran Song</span>,
                    <span>Sujay Sanghavi</span>,
                    <span>Fartash Faghri</span>,
                    <span>Sewoong Oh</span>,
                    <span>Luke Zettlemoyer</span>,
                    <span>Kyle Lo</span>,
                    <span>Alaaeldin El-Nouby</span>,
                    <span>Hadi Pouransari</span>,
                    <span>Alexander Toshev</span>,
                    <span>Stephanie Wang</span>,
                    <span>Dirk Groeneveld</span>,
                    <span>Luca Soldaini</span>,
                    <span>Pang Wei Koh</span>,
                    <span>Jenia Jitsev</span>,
                    <span>Thomas Kollar</span>,
                    <span>Alexandros G. Dimakis</span>,
                    <span>Yair Carmon</span>,
                    <span>Achal Dave</span>,
                    <span>Ludwig Schmidt</span>,
                    <span>Vaishaal Shankar</span>
                </div>
                <span class="article-date">
                    June 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>NeurIPS 2024</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2406.11794" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.datacomp.ai/dclm/" target="_blank" rel="noopener">
                        Project Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/mlfoundations/dclm" target="_blank" rel="noopener">
                        Code
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://data.commoncrawl.org/contrib/datacomp/index.html" target="_blank" rel="noopener">
                        Data
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>

<div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/contrastive_diffusion_learning.png" class="article-banner" alt="Contrastive Sequential-Diffusion Learning: An approach to Multi-Scene Instructional Video Synthesis" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2407.11814">Contrastive Sequential-Diffusion Learning: An approach to Multi-Scene Instructional Video Synthesis</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Vasco Ramos</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Michal Yarom</span>,
                    <span>Idan Szpektor</span>,
                    <span>Joao Magalhaes</span>
                </div>
                <span class="article-date">
                    July 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>WACV 2025</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2407.11814" target="_blank" rel="noopener">
                        arXiv
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>

          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/beyond_thumbs_up.png" class="article-banner" alt="Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2406.16807">Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Katherine M. Collins</span>,
                    <span>Najoung Kim</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Verena Rieser</span>,
                    <span>Shayegan Omidshafiei</span>,
                    <span>Yushi Hu</span>,
                    <span>Sherol Chen</span>,
                    <span>Senjuti Dutta</span>,
                    <span>Minsuk Chang</span>,
                    <span>Kimin Lee</span>,
                    <span>Youwei Liang</span>,
                    <span>Georgina Evans</span>,
                    <span>Sahil Singla</span>,
                    <span>Gang Li</span>,
                    <span>Adrian Weller</span>,
                    <span>Junfeng He</span>,
                    <span>Deepak Ramachandran</span>,
                    <span>Krishnamurthy Dj Dvijotham</span>
                </div>
                <span class="article-date">
                    June 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>AIES 2024</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2406.16807" target="_blank" rel="noopener">
                        arXiv
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>

          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/visual-riddles.png" class="article-banner" alt="Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2407.19474">Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Nitzan Bitton-Guetta</span>,
                    <span>Aviv Slobodkin</span>,
                    <span>Aviya Maimon</span>,
                    <span>Eliya Habba</span>,
                    <span>Royi Rassin</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Idan Szpektor</span>,
                    <span>Amir Globerson</span>,
                    <span>Yuval Elovici</span>
                </div>
                <span class="article-date">
                    July 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>NeurIPS 2024, Datasets and Benchmarks</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2407.19474" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://visual-riddles.github.io/" target="_blank" rel="noopener">
                        Project Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/spaces/visual-riddles/visual-riddles" target="_blank" rel="noopener">
                        Explorer
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>



          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <img src="/publication/videostar.png" class="article-banner" alt="VideoSTaR: Bootstrapping Weak Video Supervision for Visual Instruction Tuning" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2407.06189">VideoSTaR: Bootstrapping Weak Video Supervision for Visual Instruction Tuning</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Orr Zohar</span>,
                    <span>Xiaohan Wang</span>
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Idan Szpektor</span>,
                    <span>Serena Yeung-Levy</span>
                </div>
                <span class="article-date">
                    July 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>ICLR 2025</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2407.06189" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://orrzohar.github.io/projects/video-star/" target="_blank" rel="noopener">
                        Project Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/orrzohar/Video-STaR" target="_blank" rel="noopener">
                        GitHub
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/orrzohar/Video-STaR" target="_blank" rel="noopener">
                        Dataset
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/spaces/orrzohar/Video-STaR" target="_blank" rel="noopener">
                        Demo
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>


          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <!-- Placeholder image if available -->
            <img src="/publication/videophy.png" class="article-banner" alt="VideoPhy: Evaluating Physical Commonsense In Video Generation" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2406.03520">VideoPhy: Evaluating Physical Commonsense In Video Generation</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Hritik Bansal</span>,
                    <span>Zongyu Lin</span>,
                    <span>Tianyi Xie</span>,
                    <span>Zeshun Zong</span>,
                    <span>Michal Yarom</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Chenfanfu Jiang</span>,
                    <span>Yizhou Sun</span>,
                    <span>Kai-Wei Chang</span>,
                    <span>Aditya Grover</span>
                </div>
                <span class="article-date">
                    June 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>ICLR 2025</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2406.03520" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://videophy.github.io/" target="_blank" rel="noopener">
                        Project Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/Hritikbansal/videophy" target="_blank" rel="noopener">
                        GitHub
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/videophysics/videophy_test_public" target="_blank" rel="noopener">
                        Dataset
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/videophysics/videocon_physics/tree/main" target="_blank" rel="noopener">
                        Model Demo
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>


            <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <!-- Placeholder image if available -->
            <img src="/publication/talc/figure.png" class="article-banner" alt="TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/pdf/2405.04682">TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Hritik Bansal</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Michal Yarom</span>,
                    <span>Idan Szpektor</span>,
                    <span>Aditya Grover</span>,
                    <span>Kai-Wei Chang</span>
                </div>
                <span class="article-date">
                    May 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>arXiv preprint</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2405.04682" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://talc-mst2v.github.io/" target="_blank" rel="noopener">
                        Project Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/Hritikbansal/talc" target="_blank" rel="noopener">
                        GitHub
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/hbXNov/multi_scene_video_text_data" target="_blank" rel="noopener">
                        Dataset
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/hbXNov/talc_finetuned_modelscope_t2v" target="_blank" rel="noopener">
                        Model Demo
                    </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/talc/2405.04682v1.pdf" target="_blank" rel="noopener">
                        PDF
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>



            <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <!-- Placeholder image if available -->
            <img src="/publication/imageinwords/figure.png" class="article-banner" alt="ImageInWords: Unlocking Hyper-Detailed Image Descriptions" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2405.02793">ImageInWords: Unlocking Hyper-Detailed Image Descriptions</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Roopal Garg</span>,
                    <span>Andrea Burns</span>,
                    <span>Burcu Karagol Ayan</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Ceslee Montgomery</span>,
                    <span>Yasumasa Onoe</span>,
                    <span>Andrew Bunner</span>,
                    <span>Ranjay Krishna</span>,
                    <span>Jason Baldridge</span>,
                    <span>Radu Soricut</span>
                </div>
                <span class="article-date">
                    May 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>EMNLP 2024</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2405.02793" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://google.github.io/imageinwords/" target="_blank" rel="noopener">
                        Project Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/google/imageinwords" target="_blank" rel="noopener">
                        GitHub
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/spaces/google/imageinwords-explorer" target="_blank" rel="noopener">
                        Explorer
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/google/imageinwords" target="_blank" rel="noopener">
                        Dataset
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>


            <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <!-- Placeholder image if available -->
            <img src="/publication/docci/figure.png" class="article-banner" alt="DOCCI: Descriptions of Connected and Contrasting Images" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2404.19753">DOCCI: Descriptions of Connected and Contrasting Images</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Yasumasa Onoe</span>,
                    <span>Sunayana Rane</span>,
                    <span>Zachary Berger</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Jaemin Cho</span>,
                    <span>Roopal Garg</span>,
                    <span>Alexander Ku</span>,
                    <span>Zarana Parekh</span>,
                    <span>Jordi Pont-Tuset</span>,
                    <span>Garrett Tanzer</span>,
                    <span>et al.</span>
                </div>
                <span class="article-date">
                    April 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>ECCV 2024</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2404.19753" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://google.github.io/docci" target="_blank" rel="noopener">
                        Project Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://google.github.io/docci/viz.html?c=&p=1" target="_blank" rel="noopener">
                        Explorer
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/google/docci" target="_blank" rel="noopener">
                        Dataset
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>

            <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <!-- Placeholder image if available -->
            <img src="/publication/parallel_parc/figure.png" class="article-banner" alt="ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2403.01139">ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Oren Sultan</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Ron Yosef</span>,
                    <span>Dafna Shahaf</span>
                </div>
                <span class="article-date">
                    March 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>NAACL 2024</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2403.01139" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/parallel_parc/2403.01139v3.pdf" target="_blank" rel="noopener">
                        PDF
                    </a>
                    <!-- Additional links can be added here if needed in the future -->
                </div>
            </div>
        </div>
    </div>
</div>


            <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <!-- Placeholder image if available -->
            <img src="/publication/chain_of_thought/figure.png" class="article-banner" alt="A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2402.00559">A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>Alon Jacovi</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Bernd Bohnet</span>,
                    <span>Jonathan Herzig</span>,
                    <span>Or Honovich</span>,
                    <span>Michael Tseng</span>,
                    <span>Michael Collins</span>,
                    <span>Roee Aharoni</span>,
                    <span>Mor Geva</span>
                </div>
                <span class="article-date">
                    February 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>ACL 2024</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2402.00559" target="_blank" rel="noopener">
                        arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://cobusgreyling.medium.com/a-benchmark-for-verifying-chain-of-thought-904db5ebeefa" target="_blank" rel="noopener">
                        Blog
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/chain_of_thought/2402.00559v3.pdf" target="_blank" rel="noopener">
                        PDF
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/google/reveal" target="_blank" rel="noopener">
                        Dataset
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>

          <div class="media stream-item">
    <div class="mr-3">
        <div class="img-hover-zoom">
            <!-- Placeholder image if available -->
            <img src="/publication/figure_nova_sequences.png" class="article-banner" alt="Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks" loading="lazy">
        </div>
    </div>
    <div class="media-body">
        <div class="section-subheading article-title">
            <a href="https://arxiv.org/abs/2405.10122">Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks</a>
        </div>
        <div class="stream-meta article-metadata">
            <div class="article-metadata">
                <div>
                    <span>João Bordalo</span>,
                    <span>Vasco Ramos</span>,
                    <span>Rodrigo Valério</span>,
                    <span>Diogo Glória-Silva</span>,
                    <strong><span>Yonatan Bitton</span></strong>,
                    <span>Michal Yarom</span>,
                    <span>Idan Szpektor</span>,
                    <span>Joao Magalhaes</span>
                </div>
                <span class="article-date">
                    May 2024
                </span>
                <span class="middot-divider"></span>
                <span class="pub-publication">
                    <em><strong>ACL 2024</em></strong>
                </span>
                <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2405.10122" target="_blank" rel="noopener">
                        arXiv
                    </a>
                </div>
            </div>
        </div>
    </div>
</div>



        	<div class="media stream-item">
            <div class="mr-3">
              <a href="https://mismatch-quest.github.io/">
                <div class="img-hover-zoom"><img src="/publication/mismatch-quest/teaser.jpg" class="article-banner" alt="Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment

" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://mismatch-quest.github.io/">Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment

</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Brian Gordon*</span>,
                    <strong><span >Yonatan Bitton*</span></strong>,
                    <span >Yonatan Shafir</span>,
                    <span >Roopal Garg</span>,
                    <span >Xi Chen</span>,
                    <span >Dani Lischinski</span>,
                    <span >Daniel Cohen-Or</span>,
                    <span >Idan Szpektor</span>,
                  </div>
                  <span class="article-date">
                    Dec 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>ECCV 2024</em></strong>
                  </span>
                    <div class="btn-links">
                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2312.03766" target="_blank" rel="noopener">
                    arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://mismatch-quest.github.io/" target="_blank" rel="noopener">
                    Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/mismatch-quest/2312.03766.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                    <!-- <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
                          data-filename="/publication/whoops/cite.bib">
                      Cite
                    </a> -->
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/BrianG13/MismatchQuest" target="_blank" rel="noopener">
                      Code
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/mismatch-quest/SeeTRUE-Feedback" target="_blank" rel="noopener">
                      Dataset
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>

        	<div class="media stream-item">
            <div class="mr-3">
              <a href="https://video-con.github.io/">
                <div class="img-hover-zoom"><img src="/publication/video-con/data-generation.png" class="article-banner" alt="VideoCon: Robust Video-Language Alignment via Contrast Captions
" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://video-con.github.io/">VideoCon: Robust Video-Language Alignment via Contrast Captions
</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Hritik Bansal</span>,
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Idan Szpektor</span>,
                    <span >Kai-Wei Chang</span>,
                    <span >Aditya Grover</span>,
                  </div>
                  <span class="article-date">
                    Nov 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>CVPR 2024</em></strong>
                  </span>
                    <div class="btn-links">
                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2311.10111" target="_blank" rel="noopener">
                    arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://video-con.github.io/" target="_blank" rel="noopener">
                    Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/video-con/2311.10111.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                    <!-- <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
                          data-filename="/publication/whoops/cite.bib">
                      Cite
                    </a> -->
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/Hritikbansal/videocon" target="_blank" rel="noopener">
                      Code
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/videocon/videocon" target="_blank" rel="noopener">
                      Dataset
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/spaces/hbXNov/owl-con-demo" target="_blank" rel="noopener">
                      Demo
                    </a>
                     <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/videocon/owl-con" target="_blank" rel="noopener">
                      Model
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://visit-bench.github.io/">
                <div class="img-hover-zoom"><img src="/publication/visit-bench/1_fig1.png" class="article-banner" alt="VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://visit-bench.github.io/">VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <strong><span >Yonatan Bitton*</span></strong>,
                    <span >Hritik Bansal*</span>,
                    <span >Jack Hessel*</span>,
                    <span >Rulin Shao</span>,
                    <span >Wanrong Zhu</span>,
                    <span >Anas Awadalla</span>,
                    <span >Josh Gardner</span>,
                    <span >Rohan Taori</span>,
                    <span >Ludwig Schmidt</span>,
                  </div>
                  <span class="article-date">
                    Aug 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>NeurIPS 2023, Datasets and Benchmarks</em></strong>
                  </span>
                    <div class="btn-links">
                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2308.06595" target="_blank" rel="noopener">
                    arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://visit-bench.github.io/" target="_blank" rel="noopener">
                    Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/visit-bench/visit-bench-paper.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                    <!-- <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
                          data-filename="/publication/whoops/cite.bib">
                      Cite
                    </a> -->
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/mlfoundations/VisIT-Bench/" target="_blank" rel="noopener">
                      Code
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/mlfoundations/VisIT-Bench" target="_blank" rel="noopener">
                      Dataset
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/spaces/mlfoundations/VisIT-Bench-Leaderboard" target="_blank" rel="noopener">
                      Leaderboard
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laion.ai/blog/visit_bench" target="_blank" rel="noopener">
                    LAION Blog
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://arxiv.org/abs/2307.04532">
                <div class="img-hover-zoom"><img src="/publication/read_look_listen/fig1.png" class="article-banner" alt="Read, Look or Listen? What's Needed for Solving a Multimodal Dataset
" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://arxiv.org/abs/2307.04532">Read, Look or Listen? What's Needed for Solving a Multimodal Dataset

</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Netta Madvil</span>,
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Roy Schwartz</span>,
                  </div>
                  <span class="article-date">
                    July 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                  </span>
                   <div class="btn-links">
                                          <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2307.04532" target="_blank" rel="noopener">
                      arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/read_look_listen/2307.04532.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://twitter.com/NettaMadvil/status/1678696232242356224" target="_blank" rel="noopener">
                      Tweet
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>


          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://arxiv.org/abs/2305.15026/">
                <div class="img-hover-zoom"><img src="/publication/nl2vi/fig1.png" class="article-banner" alt="Transferring Visual Attributes from Natural Language to Verified Image Generation
" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://arxiv.org/abs/2305.15026">Transferring Visual Attributes from Natural Language to Verified Image Generation
</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Rodrigo Valerio</span>,
                    <span >Joao Bordalo</span>,
                    <span >Michal Yarom</span>,
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Idan Szpektor</span>,
                    <span >Joao Magalhaes</span>
                  </div>
                  <span class="article-date">
                    May 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                  </span>
                   <div class="btn-links">
                                          <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2305.15026" target="_blank" rel="noopener">
                      arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/nl2vi/2305.15026.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>


          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://wysiwyr-itm.github.io/">
                <div class="img-hover-zoom"><img src="/publication/wysiwyr/fig_cat.png" class="article-banner custom-banner" alt="What You See is What You Read? Improving Text-Image Alignment Evaluation
" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://wysiwyr-itm.github.io/">What You See is What You Read? Improving Text-Image Alignment Evaluation
</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div> 
                    <span >Michal Yarom*</span>,
                    <strong><span >Yonatan Bitton*</span></strong>,
                    <span >Soravit "Beer" Changpinyo</span>,
                    <span >Roee Aharoni</span>,
                    <span >Jonathan Herzig</span>,
                    <span >Oran Lang</span>
                    <span >Eran Ofek</span>
                    <span >Idan Szpektor</span>

                  </div>
                  <span class="article-date">
                    May 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>NeurIPS 2023</em></strong>
                  </span>
                   <div class="btn-links">
                                          <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2305.10400" target="_blank" rel="noopener">
                      arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://wysiwyr-itm.github.io/" target="_blank" rel="noopener">
                    Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/wysiwyr/2305.10400.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://twitter.com/YonatanBitton/status/1659089605264650245" target="_blank" rel="noopener">
                      Tweet
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>

        	<div class="media stream-item">
            <div class="mr-3">
              <a href="https://question2dialog.github.io/">
                <div class="img-hover-zoom"><img src="/publication/q2d/fig1.png" class="article-banner" alt="q2d: Turning Questions into Dialogs to Teach Models How to Search" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://question2dialog.github.io/">q2d: Turning Questions into Dialogs to Teach Models How to Search</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Shlomi Cohen-Ganor</span>,
                    <span >Ido Hakimi</span>,
                    <span >Yoad Lewenberg</span>,
                    <span >Roee Aharoni</span>,
                    <span >Enav Weinreb</span>
                  </div>
                  <span class="article-date">
                    April 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>EMNLP 2023</em></strong>
                  </span>
                   <div class="btn-links">
                                          <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2304.14318" target="_blank" rel="noopener">
                      arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://question2dialog.github.io/" target="_blank" rel="noopener">
                    Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/q2d/2304.14318.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://twitter.com/YonatanBitton/status/1651893170890760193" target="_blank" rel="noopener">
                      Tweet
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>

        	<div class="media stream-item">
            <div class="mr-3">
              <a href="https://laion.ai/blog/datacomp/">
                <div class="img-hover-zoom"><img src="/publication/data_comp/data_comp_fig1.png" class="article-banner" alt="DataComp: In search of the next generation of multimodal datasets" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://www.datacomp.ai/">DataComp: In search of the next generation of multimodal datasets
</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Samir Yitzhak Gadre</span>,
                    <span >Gabriel Ilharco</span>,
                    <span >Alex Fang</span>,
                    <span >Jonathan Hayase</span>,
                    <span >Georgios Smyrnis</span>,
                    <span >Thao Nguyen</span>,
                    <span >Ryan Marten</span>,
                    <span >Mitchell Wortsman</span>,
                    <span >Dhruba Ghosh</span>,
                    <span >Jieyu Zhang</span>,
                    <span >Eyal Orgad</span>,
                    <span >Rahim Entezari</span>,
                    <span >Giannis Daras</span>,
                    <span >Sarah Pratt</span>,
                    <span >Vivek Ramanujan</span>
                    <strong><strong><span >Yonatan Bitton</span></strong></strong>
                    <span >Kalyani Marathe</span>
                    <span >Stephen Mussmann</span>
                    <span >Richard Vencu</span>
                    <span >Mehdi Cherti</span>
                    <span >Ranjay Krishna</span>
                    <span >Pang Wei Koh</span>
                    <span >Olga Saukh</span>
                    <span >Alexander Ratner</span>
                    <span >Shuran Song</span>
                    <span >Hannaneh Hajishirzi</span>
                    <span >Ali Farhadi</span>
                    <span >Romain Beaumont</span>
                    <span >Sewoong Oh</span>
                    <span >Alex Dimakis</span>
                    <span >Jenia Jitsev</span>
                    <span >Yair Carmon</span>
                    <span >Vaishaal Shankar</span>
                    <span >Ludwig Schmidt</span>
                  </div>
                  <span class="article-date">
                    April 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication"><strong>NeurIPS 2023, Datasets and Benchmarks</strong>
                  </span>
                  <div class="btn-links">
                                          <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2304.14108" target="_blank" rel="noopener">
                      arXiv
                    </a>
                                          <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/data_comp/2304.14108.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>

                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.datacomp.ai/" target="_blank" rel="noopener">
                    Website
                    </a>

                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/mlfoundations/datacomp" target="_blank" rel="noopener">
                      GitHub
                    </a>
                     <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laion.ai/blog/datacomp/" target="_blank" rel="noopener">
                    LAION Blog
                    </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://twitter.com/YonatanBitton/status/1651980821312466944" target="_blank" rel="noopener">
                        Tweet
                      </a>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://laion.ai/blog/open-flamingo/">
                <div class="img-hover-zoom"><img src="/publication/open-flamingo/open-flamingo.jpeg" class="article-banner" alt="OpenFlamingo: An open-source framework for training vision-language models with in-context learning" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://laion.ai/blog/open-flamingo/">OpenFlamingo: An open-source framework for training vision-language models with in-context learning</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Anas Awadalla</span>,
                    <span >Irena Gao</span>,
                    <span >Joshua Gardner</span>,
                    <span >Jack Hessel</span>,
                    <span >Yusuf Hanafy</span>,
                    <span >Wanrong Zhu</span>,
                    <span >Kalyani Marathe</span>,
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Samir Gadre</span>,
                    <span >Jenia Jitsev</span>,
                    <span >Simon Kornblith</span>,
                    <span >Pang Wei Koh</span>,
                    <span >Gabriel Ilharco</span>,
                    <span >Mitchell Wortsman</span>,
                    <span >Ludwig Schmidt</span>
                  </div>
                  <span class="article-date">
                    Mar 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>NeurIPS 2023, Datasets and Benchmarks</em></strong>
                  </span>
                    <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laion.ai/blog/open-flamingo/" target="_blank" rel="noopener">
                    Website
                    </a>
<!--                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/vasr/vasr.pdf" target="_blank" rel="noopener">-->
<!--                      PDF-->
<!--                    </a>-->
<!--                    <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"-->
<!--                          data-filename="/publication/vasr/cite.bib">-->
<!--                      Cite-->
<!--                    </a>-->
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/mlfoundations/open_flamingo" target="_blank" rel="noopener">
                      Code
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/openflamingo/OpenFlamingo-9B" target="_blank" rel="noopener">
                      Hugging Face
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://arxiv.org/abs/2303.15445">
                <div class="img-hover-zoom"><img src="/publication/IRFL/IRFL_fig1.jpg" class="article-banner" alt="IRFL: Image Recognition of Figurative Language" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://irfl-dataset.github.io/">IRFL: Image Recognition of Figurative Language</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Ron Yosef</span>,
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Dafna Shahaf</span>
                  </div>
                  <span class="article-date">
                    Mar 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>Findings of EMNLP 2023</em></strong>
                  </span>
                    <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2303.15445" target="_blank" rel="noopener">
                    arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://irfl-dataset.github.io/" target="_blank" rel="noopener">
                    Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/IRFL/IRFL.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                    <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
                          data-filename="/publication/IRFL/cite.bib">
                      Cite
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://twitter.com/YonatanBitton/status/1641141392826068992" target="_blank" rel="noopener">
                    Tweet
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://whoops-benchmark.github.io/">
                <div class="img-hover-zoom"><img src="/publication/whoops/whoops_image.jpg" class="article-banner" alt="WHOOPS! A Vision-and-Language Commonsense Benchmark of Heterogeneous Objects and Situations" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://whoops-benchmark.github.io/">WHOOPS! A Vision-and-Language Commonsense Benchmark of Heterogeneous Objects and Situations</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Nitzan Bitton-Guetta*</span>,
                    <strong><span >Yonatan Bitton*</span></strong>,
                    <span >Jack Hessel</span>,
                    <span >Ludwig Schmidt</span>,
                    <span >Yuval Elovici</span>,
                    <span >Gabriel Stanovsky</span>,
                    <span >Roy Schwartz</span>
                  </div>
                  <span class="article-date">
                    Mar 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>ICCV 2023</em></strong>
                  </span>
                    <div class="btn-links">
                	<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2303.07274" target="_blank" rel="noopener">
                    arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://whoops-benchmark.github.io/" target="_blank" rel="noopener">
                    Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/whoops/WHOOPS_paper.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                    <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
                          data-filename="/publication/whoops/cite.bib">
                      Cite
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://colab.research.google.com/drive/1av7JdDk005qQL6WdAVL0kFlah7VXV0Md?usp=sharing" target="_blank" rel="noopener">
                      Evaluation Code
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/nlphuji/whoops" target="_blank" rel="noopener">
                      Hugging Face
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>
<!--          <div class="media stream-item">-->
<!--            <div class="mr-3">-->
<!--              <a href="">-->
<!--                <div class="img-hover-zoom"><img src="####" class="article-banner" alt="DataComp: In search of the next generation of multimodal datasets via data scaling" loading="lazy"></div>-->
<!--              </a>-->
<!--            </div>-->
<!--            <div class="media-body">-->
<!--              <div class="section-subheading article-title">-->
<!--                  <a href="">DataComp: In search of the next generation of multimodal datasets via data scaling</a>-->
<!--                </div>-->
<!--              <div class="stream-meta article-metadata">-->
<!--                <div class="article-metadata">-->
<!--                  <div>-->
<!--                    <strong><span >Yonatan Bitton</span></strong>,-->
<!--&lt;!&ndash;                    <span >Ron Yosef</span>,&ndash;&gt;-->
<!--&lt;!&ndash;                    <span >Eli Strugo</span>,&ndash;&gt;-->
<!--&lt;!&ndash;                    <span >Dafna Shahaf</span>,&ndash;&gt;-->
<!--&lt;!&ndash;                    <span >Roy Schwartz</span>,&ndash;&gt;-->
<!--&lt;!&ndash;                    <span >Gabriel Stanovsky</span>&ndash;&gt;-->
<!--                  </div>-->
<!--                  <span class="article-date">-->
<!--                    ? 2023-->
<!--                  </span>-->
<!--                  <span class="middot-divider"></span>-->
<!--                  <span class="pub-publication">-->
<!--                    <em><strong></em></strong>-->
<!--                  </span>-->
<!--                    <div class="btn-links">-->
<!--&lt;!&ndash;                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://vasr-dataset.github.io/" target="_blank" rel="noopener">&ndash;&gt;-->
<!--&lt;!&ndash;                    Website&ndash;&gt;-->
<!--&lt;!&ndash;                    </a>&ndash;&gt;-->
<!--&lt;!&ndash;                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/vasr/vasr.pdf" target="_blank" rel="noopener">&ndash;&gt;-->
<!--&lt;!&ndash;                      PDF&ndash;&gt;-->
<!--&lt;!&ndash;                    </a>&ndash;&gt;-->
<!--&lt;!&ndash;                    <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"&ndash;&gt;-->
<!--&lt;!&ndash;                          data-filename="/publication/vasr/cite.bib">&ndash;&gt;-->
<!--&lt;!&ndash;                      Cite&ndash;&gt;-->
<!--&lt;!&ndash;                    </a>&ndash;&gt;-->
<!--&lt;!&ndash;                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/vasr-dataset/vasr" target="_blank" rel="noopener">&ndash;&gt;-->
<!--&lt;!&ndash;                      Code&ndash;&gt;-->
<!--&lt;!&ndash;                    </a>&ndash;&gt;-->
<!--&lt;!&ndash;                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/nlphuji/vasr" target="_blank" rel="noopener">&ndash;&gt;-->
<!--&lt;!&ndash;                      Hugging Face&ndash;&gt;-->
<!--&lt;!&ndash;                    </a>&ndash;&gt;-->
<!--                  </div>-->
<!--                </div>-->
<!--              </div>-->
<!--            </div>-->
<!--          </div>-->


          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://vasr-dataset.github.io/">
                <div class="img-hover-zoom"><img src="/publication/vasr/featured.png" class="article-banner" alt="VASR: Visual Analogies of Situation Recognition" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://vasr-dataset.github.io/">VASR: Visual Analogies of Situation Recognition</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Ron Yosef</span>,
                    <span >Eli Strugo</span>,
                    <span >Dafna Shahaf</span>,
                    <span >Roy Schwartz</span>,
                    <span >Gabriel Stanovsky</span>
                  </div>
                  <span class="article-date">
                    Dec 2022
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>AAAI 2023</em></strong><br>
                    <font color="purple">🎉  Selected as Oral Presentation </font>
                  </span>
                    <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2212.04542" target="_blank" rel="noopener">
                    arXiv
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://vasr-dataset.github.io/" target="_blank" rel="noopener">
                    Website
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/vasr/vasr.pdf" target="_blank" rel="noopener">
                      PDF
                    </a>
                    <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
                          data-filename="/publication/vasr/cite.bib">
                      Cite
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/vasr-dataset/vasr" target="_blank" rel="noopener">
                      Code
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/nlphuji/vasr" target="_blank" rel="noopener">
                      Hugging Face
                    </a>
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://twitter.com/YonatanBitton/status/1602223698387378176" target="_blank" rel="noopener">
                      Tweet
                    </a>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://winogavil.github.io/">
                <div class="img-hover-zoom"><img src="/publication/winogavil/featured.png" class="article-banner" alt="WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://winogavil.github.io/">WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <strong><span >Yonatan Bitton*</span></strong>,
                    <span >Nitzan Bitton-Guetta*</span>,
                    <span >Ron Yosef</span>,
                    <span >Yuval Elovici</span>,
                    <span >Mohit Bansal</span>,
                    <span >Gabriel Stanovsky</span>,
                    <span >Roy Schwartz</span>
                  </div>
                  <span class="article-date">
                    Jul 2022
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>NeurIPS 2022, Datasets and Benchmarks</em></strong><br>
                    <font color="purple">🎉  Selected as Featured Presentation </font>
                  </span>
                    <div class="btn-links">
                    <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2207.12576" target="_blank" rel="noopener">
                    arXiv</a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://winogavil.github.io/" target="_blank" rel="noopener">
                        Website
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/winogavil/winogavil.pdf" target="_blank" rel="noopener">
                        PDF
                      </a>
                      <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
                              data-filename="/publication/winogavil/cite.bib">
                        Cite
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/WinoGAViL/WinoGAViL-experiments" target="_blank" rel="noopener">
                        Code
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/winogavil/winogavil_poster_neurips.pdf" target="_blank" rel="noopener">
                        Poster
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://paperswithcode.com/dataset/winogavil" target="_blank" rel="noopener">
                        Papers With Code
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/nlphuji/winogavil" target="_blank" rel="noopener">
                        Hugging Face
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://twitter.com/YonatanBitton/status/1552385443580321793" target="_blank" rel="noopener">
                        Tweet
                      </a>
                  </div>
                </div>
              </div>
            </div>
          </div>



          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://arxiv.org/pdf/2109.02040.pdf">
                <div class="img-hover-zoom"><img src="/publication/data_efficient/featured_hu89ddd0647d2cfdbbf6482c615d0017c6_661451_808x455_fill_lanczos_smart1_3.png" class="article-banner" alt="Data Efficient Masked Language Modeling for Vision and Language" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://arxiv.org/abs/2109.02040">Data Efficient Masked Language Modeling for Vision and Language</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Gabriel Stanovsky</span>,
                    <span >Michael Elhadad</span>,
                    <span >Roy Schwartz</span>
                  </div>
                  <span class="article-date">
                    September 2021
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>Findings of EMNLP 2021</em></strong>
                  </span>
                    <div class="btn-links">
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2109.02040" target="_blank" rel="noopener">
                        arXiv
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/data_efficient/data_efficient.pdf" target="_blank" rel="noopener">
                        PDF
                      </a>
                      <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
                              data-filename="/publication/data_efficient/cite.bib">
                        Cite
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/yonatanbitton/data_efficient_masked_language_modeling_for_vision_and_language" target="_blank" rel="noopener">
                        Code
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/data_efficient/data_efficient_poster.pdf" target="_blank" rel="noopener">
                        Poster
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=A-KOkb5CURA" target="_blank" rel="noopener">
                        Video
                      </a>
                       <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://twitter.com/YonatanBitton/status/1435945952368603143" target="_blank" rel="noopener">
                        Tweet
                      </a>
                    </div>
                </div>
              </div>
            </div>
          </div>

          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://arxiv.org/pdf/2103.09591.pdf">
                <div class="img-hover-zoom"><img src="/publication/contrast_sets/featured_hue00ceed770a4d5dafe8725e68e820d5a_789004_808x455_fill_lanczos_smart1_3.png" class="article-banner" alt="Data Efficient Masked Language Modeling for Vision and Language" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://arxiv.org/abs/2103.09591">Automatic Generation of Contrast Sets from Scene Graphs</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Gabriel Stanovsky</span>,
                    <span >Roy Schwartz</span>,
                    <span >Michael Elhadad</span>
                  </div>
                  <span class="article-date">
                    March 2021
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>NAACL 2021</em></strong>
                  </span>
                    <div class="btn-links">
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2103.09591" target="_blank" rel="noopener">
                        arXiv
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/contrast_sets/contrast_sets.pdf" target="_blank" rel="noopener">
                        PDF
                      </a>
                      <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename="/publication/contrast_sets/cite.bib">
                        Cite
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/yonatanbitton/automatic_generation_of_contrast_sets_from_scene_graphs" target="_blank" rel="noopener">
                        Code
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/contrast_sets/contrast_sets_poster.pdf" target="_blank" rel="noopener">
                        Poster
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be/IogU2qbAsm0" target="_blank" rel="noopener">
                        Video
                      </a>
                       <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://twitter.com/YonatanBitton/status/1372484049227415553" target="_blank" rel="noopener">
                        Tweet
                      </a>
                    </div>
                </div>
              </div>
            </div>
          </div>

          <div class="media stream-item">
            <div class="mr-3">
              <a href="https://academic.oup.com/jamia/article/27/10/1585/5903800">
                <div class="img-hover-zoom"><img src="/publication/cross_lingual_entity_linking/cross_fig.jpeg" class="article-banner" alt="Cross lingual Unified Medical Language System entity linking in online health communities" loading="lazy"></div>
              </a>
            </div>
            <div class="media-body">
              <div class="section-subheading article-title">
                  <a href="https://academic.oup.com/jamia/article/27/10/1585/5903800">Cross lingual Unified Medical Language System entity linking in online health communities</a>
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <strong><span >Yonatan Bitton</span></strong>,
                    <span >Raphael Cohen</span>,
                    <span >Tamar Schifter</span>,
                    <span >Eitan Bachmat</span>,
                    <span >Michael Elhadad</span>,
                    <span >Noémie Elhadad</span>,
                  </div>
                  <span class="article-date">
                    Sep 2020
                  </span>
                  <span class="middot-divider"></span>
                  <span class="pub-publication">
                    <em><strong>JAMIA 2020</em></strong>
                  </span>
                    <div class="btn-links">
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/cross_lingual_entity_linking/cross_lingual_entity_linking.pdf" target="_blank" rel="noopener">
                        PDF
                      </a>
                      <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename="/publication/cross_lingual_entity_linking/cite.bib">
                        Cite
                      </a>
                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/yonatanbitton/mdtel" target="_blank" rel="noopener">
                        Code
                      </a>
<!--                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/contrast_sets/contrast_sets_poster.pdf" target="_blank" rel="noopener">-->
<!--                        Poster-->
<!--                      </a>-->
<!--                      <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be/IogU2qbAsm0" target="_blank" rel="noopener">-->
<!--                        Video-->
<!--                      </a>-->
                    </div>
                </div>
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>



  <section id="experience" class="home-section wg-experience  "  >
   <div class="home-section-bg " >
   </div>
    <div class="container">
      <div class="row  ">
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Work Experience</h1>
          </div>
          <div class="col-12 col-lg-8">

              <div class="row experience">
              <div class="col-auto text-center flex-column d-none d-sm-flex">
                <div class="row h-50">
                  <div class="col ">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
                <div class="m-2">
                  <span class="badge badge-pill border exp-fill">&nbsp;</span>
                </div>
                <div class="row h-50">
                  <div class="col border-right">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
              </div>
              <div class="col py-2">
                <div class="card">
                  <div class="card-body">
                    <div class="d-flex align-content-start">
                      <div class="mr-2 mb-2"><img src="/media/icons/brands/google-icon.svg" width="56px" height="56px" alt="Google Research"></div>
                      <div>
                        <div class="section-subheading card-title exp-title text-muted my-0">Senior Research Scientist</div>
                        <div class="section-subheading card-title exp-company text-muted my-0">Google Research</div>
                        <div class="text-muted exp-meta">
                          April 2024 –
                            Present
                            <span class="middot-divider"></span>
                            <span>Israel</span>
                        </div>
                      </div>
                    </div>
                    <div class="card-text">Advancing multimodal consistency. Developing feedback models for text-to-image and text-to-video applications and enhance multimodal factuality to ensure the accuracy of text generated from visual sources.</div>
                  </div>
                </div>
              </div>
            </div>

<div class="row experience">
              <div class="col-auto text-center flex-column d-none d-sm-flex">
                <div class="row h-50">
                  <div class="col ">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
                <div class="m-2">
                  <span class="badge badge-pill border exp-fill">&nbsp;</span>
                </div>
                <div class="row h-50">
                  <div class="col border-right">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
              </div>
              <div class="col py-2">
                <div class="card">
                  <div class="card-body">
                    <div class="d-flex align-content-start">
                      <div class="mr-2 mb-2"><img src="/media/icons/brands/google-icon.svg" width="56px" height="56px" alt="Google Research"></div>
                      <div>
                        <div class="section-subheading card-title exp-title text-muted my-0">Research Scientist</div>
                        <div class="section-subheading card-title exp-company text-muted my-0">Google Research</div>
                        <div class="text-muted exp-meta">
                          Jun 2023 –
                            April 2024
                            <span class="middot-divider"></span>
                            <span>Israel</span>
                        </div>
                      </div>
                    </div>
                    <div class="card-text">Focusing on vision-and-language. Recent works include image-text alignment, improving text-to-image models, and visual instruction tuning.</div>
                  </div>
                </div>
              </div>
            </div>

            <div class="row experience">
              <div class="col-auto text-center flex-column d-none d-sm-flex">
                <div class="row h-50">
                  <div class="col ">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
                <div class="m-2">
                  <span class="badge badge-pill border exp-fill">&nbsp;</span>
                </div>
                <div class="row h-50">
                  <div class="col border-right">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
              </div>
              <div class="col py-2">
                <div class="card">
                  <div class="card-body">
                    <div class="d-flex align-content-start">
                      <div class="mr-2 mb-2"><img src="/media/icons/brands/google-icon.svg" width="56px" height="56px" alt="Google Research"></div>
                      <div>
                        <div class="section-subheading card-title exp-title text-muted my-0">Research Intern</div>
                        <div class="section-subheading card-title exp-company text-muted my-0">Google Research</div>
                        <div class="text-muted exp-meta">
                          Jul 2022 –
                            Jun 2023
                            <span class="middot-divider"></span>
                            <span>Israel</span>
                        </div>
                      </div>
                    </div>
                    <div class="card-text">Cerebra team, Conversational AI, working with LLMs (LaMDA, PaLM, BARD, etc)</div>
                  </div>
                </div>
              </div>
            </div>
            <div class="row experience">
              <div class="col-auto text-center flex-column d-none d-sm-flex">
                <div class="row h-50">
                  <div class="col ">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
                <div class="m-2">
                  <span class="badge badge-pill border">&nbsp;</span>
                </div>
                <div class="row h-50">
                  <div class="col border-right">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
              </div>
              <div class="col py-2">
                <div class="card">
                  <div class="card-body">
                    <div class="d-flex align-content-start">
                      <div class="mr-2 mb-2"><img src="/media/icons/brands/amazon-icon.svg" width="56px" height="56px" alt="Amazon Lab126"></div>
                      <div>
                        <div class="section-subheading card-title exp-title text-muted my-0">Applied Scientist Intern</div>
                        <div class="section-subheading card-title exp-company text-muted my-0">Amazon Lab126</div>
                        <div class="text-muted exp-meta">
                          Oct 2019 – July 2022
                            <span class="middot-divider"></span>
                            <span>Israel</span>
                        </div>
                      </div>
                    </div>
                    <div class="card-text">Visual Fitness - Halo team<br>
                      Developed a virtual fitness trainer, specializing in 2D/3D pose estimation, action recognition, error correction, on-device deployment and more.</div>
                  </div>
                </div>
              </div>
            </div>
            <div class="row experience">
              <div class="col-auto text-center flex-column d-none d-sm-flex">
                <div class="row h-50">
                  <div class="col border-right">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
                <div class="m-2">
                  <span class="badge badge-pill border ">&nbsp;</span>
                </div>
                <div class="row h-50">
                  <div class="col ">&nbsp;</div>
                  <div class="col">&nbsp;</div>
                </div>
              </div>
              <div class="col py-2">
                <div class="card">
                  <div class="card-body">
                    <div class="d-flex align-content-start">
                      <div class="mr-2 mb-2"><img src="/media/icons/brands/ibm-icon.svg" width="56px" height="56px" alt="IBM Research"></div>
                      <div>
                        <div class="section-subheading card-title exp-title text-muted my-0">Research Student</div>
                        <div class="section-subheading card-title exp-company text-muted my-0">IBM Research</div>
                        <div class="text-muted exp-meta">
                          Jun 2017 – Oct 2019
                            <span class="middot-divider"></span>
                            <span>Israel</span>
                        </div>
                      </div>
                    </div>
                    <div class="card-text">Using data-science and machine-learning methods in order to detect frauds</div>
                  </div>
                </div>
              </div>
            </div>
          </div>
      </div>
    </div>
  </section>
  
 <section id="talks" class="home-section wg-pages  "  >
    <div class="home-section-bg " ></div>
    <div class="container">
      <div class="row  ">
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Invited Talks</h1>
        </div>
        <div class="col-12 col-lg-8">
          <div class="media stream-item">
            <div class="media-body">
              <div class="article-style">
                Bridging Vision and Language with Data: From Perception to Understanding
              </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <span class="article-date">
                    April-June 2023
                  </span>
                  <span class="middot-divider"></span>
                  <span class="article-reading-time">
                    Hebrew University of Jerusalem, NLP-IL Reading Group, Microsoft Israel (MSAI-HIVE team), Meta AI Research Tel-Aviv, Technion, Ben Gurion University, Google Tel-Aviv, Bar-Ilan University, IBM Research (Israel NLP team), Tel Aviv University<br>
                    My talk 🎬 record is available <a href="https://www.youtube.com/watch?v=SR-od_Qsnvs">here</a>.
                  </span>
                </div>
              </div>
            </div>
          </div>

          <div class="media stream-item">
            <div class="media-body">
                          <div class="article-style">
                            Commonsense Benchmarks for Vision and Language
                          </div>
                          <div class="stream-meta article-metadata">
                            <div class="article-metadata">
                              <span class="article-date">
                                November 2022
                              </span>
                              <span class="middot-divider"></span>
                              <span class="article-reading-time">
                                NLP Seminar at Cornell Tech, Google Research Israel, the Hebrew University of Jerusalem
                              </span>
                            </div>
                          </div>
                        </div>
                      </div>

         <div class="media stream-item">               
            <div class="media-body">
                          <div class="article-style">
                            q2d: Turning Questions into Dialogs to Teach Models How to Search
                          </div>
                          <div class="stream-meta article-metadata">
                            <div class="article-metadata">
                              <span class="article-date">
                                September 2022
                              </span>
                              <span class="middot-divider"></span>
                              <span class="article-reading-time">
                                Conversational applications with LLMs - Summit in Google Zurich
                              </span>
                            </div>
                          </div>
                        </div>
                      </div>

          <div class="media stream-item">
             <div class="media-body">
                        <div class="article-style">
                          WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models
                        </div>
                        <div class="stream-meta article-metadata">
                          <div class="article-metadata">
                            <span class="article-date">
                              June 2022
                            </span>
                            <span class="middot-divider"></span>
                            <span class="article-reading-time">
                              IBM Research Israel
                            </span>
                          </div>
                        </div>
                      </div>
                    </div>

            <div class="media stream-item">
              <div class="media-body">
                        <div class="article-style">
                          VASR: Visual Analogies of Situation Recognition
                        </div>
                        <div class="stream-meta article-metadata">
                          <div class="article-metadata">
                            <span class="article-date">
                              May 2022
                            </span>
                            <span class="middot-divider"></span>
                            <span class="article-reading-time">
                              Computer Vision Seminar at the Hebrew University of Jerusalem
                            </span>
                          </div>
                        </div>
                      </div>
                    </div>

          </div>
        </div>
      </div>
    </div>
  </section>

   <section id="others" class="home-section wg-pages  "  >
    <div class="home-section-bg " ></div>
    <div class="container">
      <div class="row  ">
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Others</h1>
        </div>
        <div class="col-12 col-lg-8">
          <div class="media stream-item">
            <div class="media-body">
              <div class="section-subheading article-title mb-0 mt-0">
                <a href="https://yonatanbitton.github.io/uploads/managing_research.pdf" >Managing Research</a>
              </div>
                <div class="article-style">
                  This talk deals with several research related questions. For example findings new research ideas, choose a research topic, staying updated with new research, working with your supervisors, and more.
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Yonatan Bitton</span>
                  </div>
                  <span class="article-date">
                    Jan 5, 2022
                  </span>
                  <span class="middot-divider"></span>
                </div>
              </div>
            </div>
            <div class="ml-3">
              <a href="https://yonatanbitton.github.io/uploads/managing_research.pdf" >
                <img src="/post/managing_research/featured_hub5b4b50c774fc1c76bbf2efa53919e8f_828066_150x0_resize_lanczos_3.png" alt="Managing Research" loading="lazy">
              </a>
            </div>
          </div>
          <div class="media stream-item">
            <div class="media-body">
              <div class="section-subheading article-title mb-0 mt-0">
                <a href="https://www.youtube.com/watch?v=5uM_oO-kha4&t=185s" >AirPal</a>
              </div>
                <div class="article-style">
                  A platform that connects drone pilots with people in need of drone services.<br> 
                  This project participated in Starter - Jump course and won 1st place in the final Demo Day event.<br> 
                  Press coverage: <a href="https://www.telecomnews.co.il/%D7%A1%D7%98%D7%90%D7%A8%D7%98%D7%90%D7%A4-%D7%A0%D7%95%D7%9C%D7%93-%D7%9E%D7%99-%D7%94%D7%9D-%D7%94%D7%99%D7%96%D7%9E%D7%99%D7%9D-%D7%95%D7%94%D7%9E%D7%99%D7%96%D7%9E%D7%99%D7%9D-%D7%A9%D7%96%D7%9B%D7%95-%D7%91%D7%9E%D7%90%D7%99%D7%A5-%D7%94%D7%A1%D7%98%D7%95%D7%93%D7%A0%D7%98%D7%99%D7%9D-%D7%94%D7%A8%D7%90%D7%A9%D7%95%D7%9F-%D7%91%D7%A0%D7%92%D7%91.html">telecomnews</a>, <a href="https://www.israeldefense.co.il/node/34558">israeldefense</a>, <a href="https://sheva7.co.il/news/119357/">sheva7</a>.
                </div>
              <div class="stream-meta article-metadata">
                <div class="article-metadata">
                  <div>
                    <span >Yonatan Bitton</span>
                  </div>
                  <span class="article-date">
                    June, 2018
                  </span>
                  <span class="middot-divider"></span>
                </div>
              </div>
            </div>
            <div class="ml-3">
              <a href="https://www.youtube.com/watch?v=FRv4veT8xoY" >
                <img src="/post/airpal/airpal.png" alt="AirPal" loading="lazy">
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<!--  <section id="others" class="home-section wg-pages  "  >-->
<!--     <div class="home-section-bg " >-->
<!--     </div>-->
<!--      <div class="container">-->
<!--        <div class="row  ">-->
<!--            <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">-->
<!--              <h1 class="mb-0">Others</h1>-->
<!--            </div>-->
<!--            <div class="col-12 col-lg-8">-->
<!--              <div class="card-simple">-->
<!--                <div class="ml-3">-->
<!--                  <a href="/post/managing_research/" >-->
<!--                    <img src="/post/managing_research/featured_hub5b4b50c774fc1c76bbf2efa53919e8f_828066_150x0_resize_lanczos_3.png" alt="Managing Research" loading="lazy">-->
<!--                  </a>-->
<!--                </div>-->
<!--                <div class="article-metadata">-->
<!--                  <div>-->
<!--                    <strong><span >Yonatan Bitton</span></strong>,-->
<!--                    <span >Ron Yosef</span>,-->
<!--                    <span >Eli Strugo</span>,-->
<!--                    <span >Dafna Shahaf</span>,-->
<!--                    <span >Roy Schwartz</span>,-->
<!--                    <span >Gabriel Stanovsky</span>-->
<!--                  </div>-->
<!--                  <span class="article-date">-->
<!--                    Dec 2022-->
<!--                  </span>-->
<!--                  <span class="middot-divider"></span>-->
<!--                  <span class="pub-publication">-->
<!--                    In <em><strong>AAAI 2023</em></strong>-->
<!--                  </span>-->
<!--                </div>-->
<!--                <a href="/publication/vasr/">-->
<!--                <div class="img-hover-zoom"><img src="/publication/vasr/featured.png" class="article-banner" alt="VASR: Visual Analogies of Situation Recognition" loading="lazy"></div>-->
<!--                </a>-->
<!--                <div class="section-subheading article-title mb-1 mt-3">-->
<!--                  <a href="/publication/vasr/">VASR: Visual Analogies of Situation Recognition</a>-->
<!--                </div>-->
<!--                <div class="btn-links">-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://vasr-dataset.github.io/" target="_blank" rel="noopener">-->
<!--                  Website-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/vasr/vasr.pdf" target="_blank" rel="noopener">-->
<!--                    PDF-->
<!--                  </a>-->
<!--                  <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"-->
<!--                        data-filename="/publication/vasr/cite.bib">-->
<!--                    Cite-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/vasr-dataset/vasr" target="_blank" rel="noopener">-->
<!--                    Code-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/nlphuji/vasr" target="_blank" rel="noopener">-->
<!--                    Hugging Face-->
<!--                  </a>-->
<!--                </div>-->
<!--              </div>-->
<!--              <div class="card-simple">-->
<!--                <div class="article-metadata">-->
<!--                  <div>-->
<!--                    <strong><span >Yonatan Bitton</span></strong>,-->
<!--                    <span >Nitzan Bitton-Guetta</span>,-->
<!--                    <span >Ron Yosef</span>,-->
<!--                    <span >Yuval Elovici</span>,-->
<!--                    <span >Mohit Bansal</span>,-->
<!--                    <span >Gabriel Stanovsky</span>,-->
<!--                    <span >Roy Schwartz</span>-->
<!--                  </div>-->
<!--                  <span class="article-date">-->
<!--                    Jul 2022-->
<!--                  </span>-->
<!--                  <span class="middot-divider"></span>-->
<!--                  <span class="pub-publication">-->
<!--                    In <em><strong>NeurIPS 2022</em></strong> (Selected as Oral Presentation)-->
<!--                  </span>-->
<!--                </div>-->
<!--                <a href="/publication/winogavil/">-->
<!--                  <div class="img-hover-zoom"><img src="/publication/winogavil/featured.png" class="article-banner" alt="WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models" loading="lazy"></div>-->
<!--                </a>-->
<!--                <div class="section-subheading article-title mb-1 mt-3">-->
<!--                  <a href="/publication/winogavil/">WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models</a>-->
<!--                </div>-->
<!--  &lt;!&ndash;              <a href="/publication/winogavil/" class="summary-link">&ndash;&gt;-->
<!--  &lt;!&ndash;                <div class="article-style">&ndash;&gt;-->
<!--  &lt;!&ndash;                  <p>We introduce WinoGAViL: an online game to collect vision-and-language associations, (e.g., werewolves to a full moon), used as a dynamic benchmark to evaluate state-of-the-art models. Inspired by the popular card game Codenames, a spymaster gives a textual cue related to several visual candidates, and another player has to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (>90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient.</p>&ndash;&gt;-->
<!--  &lt;!&ndash;                </div>&ndash;&gt;-->
<!--  &lt;!&ndash;              </a>&ndash;&gt;-->
<!--                <div class="btn-links">-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://winogavil.github.io/" target="_blank" rel="noopener">-->
<!--                    Website-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/winogavil/winogavil.pdf" target="_blank" rel="noopener">-->
<!--                    PDF-->
<!--                  </a>-->
<!--                  <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"-->
<!--                          data-filename="/publication/winogavil/cite.bib">-->
<!--                    Cite-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/WinoGAViL/WinoGAViL-experiments" target="_blank" rel="noopener">-->
<!--                    Code-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/winogavil/winogavil_poster_neurips.pdf" target="_blank" rel="noopener">-->
<!--                    Poster-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://paperswithcode.com/dataset/winogavil" target="_blank" rel="noopener">-->
<!--                    Papers With Code-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huggingface.co/datasets/nlphuji/winogavil" target="_blank" rel="noopener">-->
<!--                    Hugging Face-->
<!--                  </a>-->
<!--                </div>-->
<!--              </div>-->
<!--              <div class="card-simple">-->
<!--                <div class="article-metadata">-->
<!--                  <div>-->
<!--                    <span >-->
<!--                        Yonatan Bitton</span>, <span >-->
<!--                        Gabriel Stanovsky</span>, <span >-->
<!--                        Michael Elhadad</span>, <span >-->
<!--                        Roy Schwartz</span>-->
<!--                  </div>-->
<!--                  <span class="article-date">-->
<!--                    September 2021-->
<!--                  </span>-->
<!--                  <span class="middot-divider"></span>-->
<!--                  <span class="pub-publication">-->
<!--                    In <em><strong>Findings of EMNLP 2021</em></strong>-->
<!--                  </span>-->
<!--                </div>-->
<!--                <a href="/publication/data_efficient/">-->
<!--                  <div class="img-hover-zoom"><img src="/publication/data_efficient/featured_hu89ddd0647d2cfdbbf6482c615d0017c6_661451_808x455_fill_lanczos_smart1_3.png" class="article-banner" alt="Data Efficient Masked Language Modeling for Vision and Language" loading="lazy"></div>-->
<!--                </a>-->
<!--                <div class="section-subheading article-title mb-1 mt-3">-->
<!--                  <a href="/publication/data_efficient/">Data Efficient Masked Language Modeling for Vision and Language</a>-->
<!--                </div>-->
<!--  &lt;!&ndash;              <a href="/publication/data_efficient/" class="summary-link">&ndash;&gt;-->
<!--  &lt;!&ndash;                <div class="article-style">&ndash;&gt;-->
<!--  &lt;!&ndash;                  <p>Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in this setting. First, as captions tend to be short, in a third of the sentences no token is sampled. Second, the majority of masked tokens are stop-words and punctuation, leading to under-utilization of the image. We investigate a range of alternative masking strategies specific to the cross-modal setting that address these shortcomings, aiming for better fusion of text and image in the learned representation. Our pre-train masking strategy consistently improves over the baseline strategy in two evaluation setups.</p>&ndash;&gt;-->
<!--  &lt;!&ndash;                </div>&ndash;&gt;-->
<!--  &lt;!&ndash;              </a>&ndash;&gt;-->
<!--                <div class="btn-links">-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/data_efficient/data_efficient.pdf" target="_blank" rel="noopener">-->
<!--                    PDF-->
<!--                  </a>-->
<!--                  <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"-->
<!--                          data-filename="/publication/data_efficient/cite.bib">-->
<!--                    Cite-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/yonatanbitton/data_efficient_masked_language_modeling_for_vision_and_language" target="_blank" rel="noopener">-->
<!--                    Code-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/data_efficient/data_efficient_poster.pdf" target="_blank" rel="noopener">-->
<!--                    Poster-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=A-KOkb5CURA" target="_blank" rel="noopener">-->
<!--                    Video-->
<!--                  </a>-->
<!--                </div>-->
<!--              </div>-->
<!--              <div class="card-simple">-->
<!--                <div class="article-metadata">-->
<!--                  <div>-->
<!--                    <span >-->
<!--                      Yonatan Bitton</span>, <span >-->
<!--                      Gabriel Stanovsky</span>, <span >-->
<!--                      Roy Schwartz</span>, <span >-->
<!--                      Michael Elhadad-->
<!--                  </span>-->
<!--                  </div>-->
<!--                  <span class="article-date">-->
<!--                    March 2021-->
<!--                  </span>-->
<!--                  <span class="middot-divider"></span>-->
<!--                  <span class="pub-publication">-->
<!--                    In <em><strong>NAACL 2021</em></strong>-->
<!--                  </span>-->
<!--                </div>-->
<!--                <a href="/publication/contrast_sets/">-->
<!--    <div class="img-hover-zoom"><img src="/publication/contrast_sets/featured_hue00ceed770a4d5dafe8725e68e820d5a_789004_808x455_fill_lanczos_smart1_3.png" class="article-banner" alt="Automatic Generation of Contrast Sets from Scene Graphs" loading="lazy"></div>-->
<!--  </a>-->
<!--                <div class="section-subheading article-title mb-1 mt-3">-->
<!--    <a href="/publication/contrast_sets/">Automatic Generation of Contrast Sets from Scene Graphs</a>-->
<!--  </div>-->
<!--  &lt;!&ndash;              <a href="/publication/contrast_sets/" class="summary-link">&ndash;&gt;-->
<!--  &lt;!&ndash;                <div class="article-style">&ndash;&gt;-->
<!--  &lt;!&ndash;                  <p>We present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models' performance on various semantic aspects (e.g., spatial or relational reasoning)</p>&ndash;&gt;-->
<!--  &lt;!&ndash;                </div>&ndash;&gt;-->
<!--  &lt;!&ndash;              </a>&ndash;&gt;-->
<!--                <div class="btn-links">-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/contrast_sets/contrast_sets.pdf" target="_blank" rel="noopener">-->
<!--                    PDF-->
<!--                  </a>-->
<!--                  <a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename="/publication/contrast_sets/cite.bib">-->
<!--                    Cite-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/yonatanbitton/automatic_generation_of_contrast_sets_from_scene_graphs" target="_blank" rel="noopener">-->
<!--                    Code-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/contrast_sets/contrast_sets_poster.pdf" target="_blank" rel="noopener">-->
<!--                    Poster-->
<!--                  </a>-->
<!--                  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be/IogU2qbAsm0" target="_blank" rel="noopener">-->
<!--                    Video-->
<!--                  </a>-->
<!--                </div>-->
<!--              </div>-->
<!--            </div>-->
<!--        </div>-->
<!--      </div>-->
<!--  </section>-->

  </div>

  <div class="page-footer">
    <div class="container">
      <footer class="site-footer">
<!--        <p class="powered-by">-->
<!--          -->
<!--        </p>-->
        <p class="powered-by" style="font-size:9.9px">
          Icons made by <a href="https://www.flaticon.com/authors/smashicons" title="Smashicons">Smashicons</a> from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a>. Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
        </p>
      </footer>
    </div>
  </div>
  
  <div id="modal" class="modal fade" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <h5 class="modal-title">Cite</h5>
          <button type="button" class="close" data-dismiss="modal" aria-label="Close">
            <span aria-hidden="true">&times;</span>
          </button>
        </div>
        <div class="modal-body">
          <pre><code class="tex hljs"></code></pre>
        </div>
        <div class="modal-footer">
          <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
            <i class="fas fa-copy"></i> Copy
          </a>
          <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
            <i class="fas fa-download"></i> Download
          </a>
          <div id="modal-error"></div>
        </div>
      </div>
    </div>
  </div>

  <script src="/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" crossorigin="anonymous"></script>
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
  <script src="/en/js/wowchemy.min.d68ecd57c0ec1f1f61d65fd568f1c3a0.js"></script>
</body>
</html>
